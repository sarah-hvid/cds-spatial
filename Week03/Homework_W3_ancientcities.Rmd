---
title: "Ancient cities and inscriptions"
author: "Adela Sobotkova"
date: "13/01/2021 updated `r format(Sys.time(), '%B %d, %Y')`" 
output:
  rmdformats::readthedown:
  highlight: kate
---

```{r setup, include=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
               cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

In this exercise you will map the ancient equivalent of Twitter data: the ancient inscriptions. Ancient people of class, education, and means liked to advertise their achievements and life milestones as well as their sorrows via the means of texts inscribed in stone. These epigraphic monuments were often placed near inhabited areas, roads, and gathering places where they were likely to attract the largest audience. The location of these self-expressions in space and time is a reasonable indicator of changing economic prosperity of the commissioning communities. In this exercise, you will explore how these ancient inscriptions spatially correspond to the distribution of ancient cities and settlements.  

```{r libraries, include=FALSE}
library(sf)
library(raster)
library(tidyverse)
library(leaflet)
```

# Task 1: Load ancient cities and convert to sf object
John Hanson has created a dataset of all cities in the ancient Mediterranean and made it available online. You will download this dataset and convert it into an sf object in order to compare with the inscriptions on the basis of location.  

* Use `read_csv()` to load `Hanson2016_Cities_OxREP.csv` dataset from the provided URL and assign it to `cities` object

```{r load-cities, eval=FALSE}
cities <- as.data.frame(read_csv("http://oxrep.classics.ox.ac.uk/oxrep/docs/Hanson2016/________________"))
```

... then reproject this data to EPSG 3035
```{r prj-cities, eval=FALSE}
# Convert the table into an sf object on the basis of X and Y columns
cities_sf <- ________(cities, coords = c("Longitude (X)", "Latitude (Y)"))

# Define the projection of Lat/Long coordinates as EPSG 4326
cities_sf4326<- st_set_crs(cities_sf, ________)

# Transform the projection to a 2D projection using EPSG 3035
cities_sf3035<- st_transform(cities_sf4326, ___________)

# Verify the projection is 'projected' not 'geographic'
______(cities_sf3035)
```


### Question: 
*1. What are the measurement units of the `cities_sf3035` object?*


# Task 2: Create a buffer around each city and inspect the result

As each city and inscription corresponds to a dot on the map, the best way to grab and review the inscriptions will be by creating a buffer around each city and then selecting the inscriptions that fall into the buffer as belonging to the city. 

* Create a buffer around the `cities_sf3035` geometry with `st_buffer()` , setting the `dist` argument to the desired radius of 5000m.
* Plot the first ten buffers and the first ten cities on top for a quick review. Can you tell that the buffers are really 5000m?

```{r buff, eval=FALSE}
# Make buffer of 5 km. Check the units of your object to correctly assign value to dist
cities_5km<- st_buffer(_____,dist = _________)

# Plot the first 10 buffers and cities to check result 
plot(___________(___________)[1:10], col = "yellow")
plot(___________(___________)[1:10], pch=20, cex = 0.1, add = TRUE)

```


# Task 3: Verify the city buffers are indeed 5km in radius
Well, a quick review may look ok, but you cannot be sure your buffers work well until you add them to a map with a scale. Verify that your buffers are as big as should be by plotting a sample with tmap and adding a scale.

* Grab the first 10 cities and buffers with the `slice()` function (if you have not already done so above)
* Load `tmap` package and plot the 10 cities and buffers with a scale of 0,5,10,20,40 km. Add names and background for clarity. Do your buffers span 10km across or do they span the universe? (If the latter, recheck your CRS, units, and dist argument)

```{r tmap, eval=FALSE}
# Grab the first 10 elements in the sf object and the buffer
ten_buffers <- ________ %>% slice(1:10)
ten_cities <- ________ %>% slice(1:10)

# Create a quick tmap
library(tmap)
current.mode <- tmap_mode("plot")

tm_shape(ten_buffers)  +
  tm_polygons(col = ________) +
  tm_shape(ten_cities) +
  tm_text("Ancient Toponym", size = ___, auto.placement = 5) +
  tm_dots(col = __________, 
             size = 0.1) +
 tm_scale_bar(breaks = ______________,
               text.size = 16,
               position = c("LEFT", "bottom")) +
  tm_compass(position = c("_________", "top"),
             type = "rose", 
             size = 2) +
  tm_credits(position = c(__________),
             text = ________________) +
  tm_layout(main.title = "Map with a scale",
            bg.color = "beige",
            inner.margins = c(0, 0, 0, 0))

```

If all went well, you should see a map, where the diameter of each city buffer corresponds to the 10km notch on the scale. But, do you know where in the Mediterranean you are?

## An Alternative View
The `tmap` package allows you also an interactive view if you switch the `tmap_mode()` argument to `"view"`. Try it out and assess its (dis)advantages. Which arguments are retained and which are dropped from your `tm_shape` sequence? 

```{r tmap-view, eval = FALSE}
current.mode <- tmap_mode(___________)
tm_shape(_________) +
  ........

```


Now you should be able to see that the first ten towns and buffers are from mainland Greece. This flexibility of tmaps is very useful.
            
# Task 4: Download ancient inscriptions and wrangle coordinates into shape 
Let's now look at some data that spatially co-occurs with these ancient places. Below is a link to an online dataset from the Epigraphic Database of Heidelberg of ancient inscriptions from one part of the Mediterranean. These inscriptions combine private and official statements dedicated for personal reasons (commemorating a patron or a family member) or public (dedication of a major building, placement of milestone, etc.). 

The json dataset is hefty with some 12 thousand inscriptions and 74 variables. Coordinates are nested in a single column and may need wrangling. Do tasks deliberately in small steps after you test on subsets lest you overwhelm your R.

* Download the linked file with `download.file()` into a directory where you can find it. 
* The inscriptions dataset is in `.json` format  (not to be confused with geojson!), which is becoming the dominant format for sharing data online, especially if some of the data is nested as is the case here. Use the `jsonlite::fromJSON` function in the library to load it back into R
* Next, use `as_tibble()` to convert into rectangular format.  
* Inspect the dataset
* Check the column names for something that looks like spatial data, either Lat/Long, X/Y or a `coordinates` column.

  - Inspect the column(s) - are the coordinates meaningful? What CRS do they look like? 
  - If their meaning is clear, are they in the format you need them to be for an easy conversion to an sf object?
* Separate the two values into `longitude` and `latitude` columns and convert values to numbers. Although this looks like a straightforward task of separating the two values into separate columns, you will see that there are non-numeric characters present that need to be cleaned up en route. (The column actually is formatted as c(123.4567,123.4577)) Make sure to keep the decimal point. Hint: check the `gsub()`, `grep()` and `str_extract()` functions to apply regular expressions.


```{r inscriptions, eval=FALSE}
# Libraries
library(tidyverse)
library(jsonlite)
library(tidytext)

# Download the file and save as inscriptions.json (consider commenting out after you first run to avoid repeat downloading)
download.file("https://sciencedata.dk/public/b6b6afdb969d378b70929e86e58ad975/EDH_subset_2021-02-15.json", "________/inscriptions.json")

# Load it into R from wherever you put it, and convert into a tibble
list_json <- jsonlite::fromJSON("_______/inscriptions.json")
inscriptions = as_tibble(list_json)

# Check the first couple lines and column names
____(inscriptions)
____(inscriptions)
head(unlist(inscriptions$coordinates))
inscriptions$coordinates[[1]][2]

# Wrangle the coordinates into a 2-column format - practice on a small dataset
i_sm <- inscriptions %>% 
  slice(1:100) %>% 
  separate(col = coordinates, into = c("longitude","latitude"), sep = ",") %>%
  mutate(latitude = as.numeric(gsub("_________","",latitude)),
         longitude = as.numeric(gsub("_________","",longitude))) 

# Apply the pipeline to the whole dataset, once happy with the result



# Check the result of the subset, does the location look reasonable?
leaflet() %>% addTiles() %>% addMarkers(lng=i$longitude,lat=i$latitude)
```

Oooof. That was some serious wrangling!

### Question: 
*2. Which part of the world are the inscriptions from?*


# Task 5: Convert inscriptions into an sf object
Now that the hard work is done, let's apply the wrangling to the full dataset and clean up the missing coordinates and outlier values.

* Not all coordinates are complete. Remove the rows with missing latitude or longitude
* Some incorrect points have sneaked in! Eliminate data with longitude smaller than 5 and larger than 20 degrees.
* Make the resulting `inscriptions` tibble into an sf object using the newly created and cleaned longitude and latitude column in the `coords` argument. The CRS of the data is 4326.
* Plot your data using st_geometry()

```{r insc-sf, eval=FALSE}
i <- inscriptions %>% 
  separate( __________) %>% 
  mutate(___________) %>% 
  filter(____________)) %>% 
  filter(__________) %>% 
  filter(longitude > 5 && longitude < 20)

# Check longitude range
hist(i$___________)

# Create a sf object
insc_sf4326 <- st_as_sf(i, coords = ___________, crs = _________)
```

### Question: 
*3. Why are we using EPSG 4326 as the value for CRS in the inscriptions? *


# Task 6: Select inscriptions that fall into the cities' buffer
Now that you have both the cities and inscriptions in the same CRS, you can pick the inscriptions which fall within 5km radius of the ancient places in order to locate "urban" inscriptions. Use the inverse `st_difference` to locate "rural" inscriptions.

To reduce the computational intensity of the final intersection, it is a good idea to limit the dissolved city buffer object only to the area within the convex hull of the inscriptions. For the convex hull, you will need to combine the inscriptions into a MULTIPOINT feature using `st_union()`.

* Ensure that the spatial reference system in `cities_5km` buffer object and `insc_sf4326` is consistent.
* Create a convex hull for the inscriptions and use it to clip the city buffers object. Check the metadata of the `cities_it` object
* Combine the city buffers into a single multipolygon
* Use `st_intersection()` to select only the inscriptions that fall within the buffer object and assign these `insc_urban` object
* Use `st_difference()`  to select inscriptions outside these buffers and create `insc_rural` object
* Plot and inspect the results: are the rural and urban inscriptions where you would expect them?

```{r intersection, eval=FALSE}
# Project the sf object into EPSG3035 so it is consistent with cities and their buffers
insc_sf3035 <- st_transform(________________)


# Create a convex hull around the inscriptions's points dissolved into a MULTIPOINT
insc_ch <- st_convex_hull(_________(insc_sf3035))

# Select cities that fall within the convex hull of the inscriptions
cities_it <- st_intersection(insc_ch, _________(cities_5km))

# Dissolve the 399 city buffers into a single MULTIPOLYGON buffer feature
c_buff <- st_union(cities_it)

# Plot these interim results
plot(___________)
plot(___________, border='red', lwd=2, add = TRUE)
plot(___________, add = TRUE)

# Calculate the number of inscriptions in urban and rural areas. This may take a couple seconds
insc_urban <- ____________(insc_sf3035, c_buff)
insc_rural <- ____________(insc_sf3035, c_buff)
```


### Question: 
*4. What is the ratio of urban to rural inscriptions?*

 
# Task 7: CHALLENGE - Duplicates and distance to nearest city

Selecting all urban inscriptions by a united buffer object should work swimmingly if you reduce computational intensity. The result of urban and rural inscriptions should add up to the total inscriptions for Italy. 
However, what if you wanted to compare one city against another in a central Italian region where cities are near one another and their buffers overlap, e.g. Rome versus Ostia? Some of the inscriptions may in such case be counted twice. The best way to eliminate duplicates is to select inscriptions on the basis of Voronyi polygons instead of buffers. But before we rush to another solution, it is perhaps best to first investigate whether such approach is necessary.

Additionally , it's a good idea to check the average distance between the inscriptions and nearest cities (points) for all the cities within the convex hull to see how far from urban centers the inscriptions are on average. Would a small change to the buffer distance dramatically change the urban:rural ratio ?

*Instructions for duplicates*

* Use the `st_intersects()` function on the POLYGON feature of 399 individual buffers and the inscriptions to get a list of inscriptions per each of the 399 buffers. * Check the class of the list object. It should be a `sgbp` object, which is a “Sparse Geometry Binary Predicate”. It is a so-called sparse matrix, which is a list with integer vectors only holding the indices for each point that intersects the individual buffer polygon. 
* Calculate how many duplicates there are. (hint: `unique()` and `unlist()` functions can help you here). Just as a thought exercise, how would you get around the duplicates?

```{r}
## YOUR CODE
```


*Instructions for mean and median distance*

* Ensure the cities object has the same CRS as inscriptions object.
* Clip or select only those cities that fall within the convex hull of inscriptions to reduce the number of calculations. 
* Calculate the mean distance between the inscriptions and cities with `st_distance()`. Reduce the cities object only to those within the convex hull of inscriptions so as to reduce the calculation. Check this documentation to understand the output of the `st_nearest_feature()` function 
https://r-spatial.github.io/sf/reference/st_nearest_feature.html as to reduce computational intensity, it's best to check the distance between the nearest features only, rather than between all inscriptions and all cities.
* run `summary()` on the product of `st_distance()` to see the mean and median distance


```{r}
## YOUR CODE
```


### Questions: 

*5. How big is the problem of overcounting? *

*6. What is the average distance of all inscriptions from all the cities within the convex hull?*

*7. What can you say about the spatial distribution of ancient inscriptions vis-a-vis the cities? What factors might be impacting the distribution?*


# Task 8: CHALLENGE - Make a pretty map!
You have 12,000 inscriptions and 399 cities in a well-known area of the world. Can you make a nice map that exposes some aspect of the data? 
You do not need to keep all the data nor both categories, and you can download more layers (you know from where). You can work with Leaflet, tmap, and you can also play with ggplot and geom_sf(). Sky and legibility are the limits :)
Have fun!

```{r}
## YOUR CODE
```

